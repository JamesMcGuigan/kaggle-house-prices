{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Competition - House Prices: Advanced Regression Techniques\n",
    "# Sklearn linear_models\n",
    "\n",
    "## Ridge, Lasso and Elastic Net \n",
    "Lets explore a range of other models available in sklearn.linear_model such as: Ridge, Lasso and Elastic Net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append( os.path.abspath( os.path.join(os.getcwd(), \"..\" ))) \n",
    "from src.utils import reset_root_dir\n",
    "reset_root_dir()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import simplejson\n",
    "import cytoolz\n",
    "import itertools\n",
    "import pydash\n",
    "import math\n",
    "import pprint\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from operator import itemgetter\n",
    "\n",
    "from src.utils.Charts import Charts\n",
    "from src.models import LinearRegressionModel, FeatureEncoding\n",
    "from src.models import RegularizationModel, RegularizationModelLinear, RegularizationModelFeatures, RegularizationModelPolynomial\n",
    "from src.models import MultiModelLinear, MultiModelFeatures, MultiModelPolynomial\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "\n",
    "pp = pprint.PrettyPrinter(depth=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for auto-reloading external modules - http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we start by looking at Ridge, Lasso and ElasticNet(Ridge+Lasso) we find the following results:\n",
    "- Ridge + FeaturesEncoding provides the best score \n",
    "- Ridge always improves relative scores against LinearRegression\n",
    "- Lasso and ElasticNet both worsen scores (against FeaturesEncoding + Linear)\n",
    "- With PolynomialFeatures, all regularization methods dramatically improve the score (due to very high number of features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.162, 'RegularizationModelFeatures', 'RidgeCV'),\n",
       " (0.1823, 'RegularizationModelFeatures', 'LinearRegression'),\n",
       " (0.1929, 'RegularizationModelLinear', 'RidgeCV'),\n",
       " (0.1942, 'RegularizationModelLinear', 'LinearRegression'),\n",
       " (0.2128, 'RegularizationModelLinear', 'LassoCV'),\n",
       " (0.2128, 'RegularizationModelFeatures', 'LassoCV'),\n",
       " (0.2242, 'RegularizationModelFeatures', 'ElasticNetCV'),\n",
       " (0.2245, 'RegularizationModelLinear', 'ElasticNetCV'),\n",
       " (0.3818, 'RegularizationModelPolynomial', 'RidgeCV'),\n",
       " (0.3929, 'RegularizationModelPolynomial', 'ElasticNetCV'),\n",
       " (0.3929, 'RegularizationModelPolynomial', 'LassoCV'),\n",
       " (0.6264, 'RegularizationModelPolynomial', 'LinearRegression')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simplified selection of first-choice models\n",
    "from property_cached import cached_property\n",
    "from typing import Dict\n",
    "from sklearn.linear_model.base import LinearModel\n",
    "from src.models import LinearRegressionModel, FeatureEncoding, PolynomialFeatureEncoding\n",
    "\n",
    "class RegularizationModel():\n",
    "    @cached_property\n",
    "    def models( self ) -> Dict[str, LinearModel]:\n",
    "        return {\n",
    "            \"LinearRegression\":  linear_model.LinearRegression(), \n",
    "            \"ElasticNetCV\":      linear_model.ElasticNetCV(cv=5),   \n",
    "            \"LassoCV\":           linear_model.LassoCV(cv=5),        \n",
    "            \"RidgeCV\":           linear_model.RidgeCV(cv=5),        \n",
    "        }\n",
    "\n",
    "class RegularizationModelLinear(     RegularizationModel, LinearRegressionModel     ): pass\n",
    "class RegularizationModelFeatures(   RegularizationModel, FeatureEncoding           ): pass\n",
    "class RegularizationModelPolynomial( RegularizationModel, PolynomialFeatureEncoding ): pass\n",
    "\n",
    "sorted(pydash.flatten([\n",
    "    RegularizationModelLinear().model_scores_list(),\n",
    "    RegularizationModelFeatures().model_scores_list(),\n",
    "    RegularizationModelPolynomial().model_scores_list(),    \n",
    "]), key=itemgetter(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit to Kaggle\n",
    "https://www.kaggle.com/c/house-prices-advanced-regression-techniques/submissions\n",
    "\n",
    "```\n",
    "$ kaggle competitions submit -c house-prices-advanced-regression-techniques -f ./data/submissions/RidgeFeatures.csv -m \"RidgeFeatures = RidgeCV + FeatureEncoding\"\n",
    "```\n",
    "    \n",
    "- Your submission scored 0.17628, which is an improvement of your previous score of 0.20892. Great job!\n",
    "- Kaggle Rank 3470 / 4375"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Model \n",
    "\n",
    "A more exhaustive search of sklearn.linear_model requires 3h runtime, but produces the following:\n",
    "- LarsCV is a good/quick all rounder and best in class for Linear + PolynomialFeatures (and outperforms Ridge)\n",
    "- ARDRegression / BayesianRidge are slow but produce the best overall result for FeaturesEncoding\n",
    "- Good performers for this dataset: LassoLars, Ridge, LassoLarsIC\n",
    "- Worst performer for this dataset: SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A more exhaustive search over options in sklearn.linear_model\n",
    "class MultiModel():\n",
    "    @cached_property\n",
    "    def models( self ) -> Dict[str, LinearModel]:\n",
    "        return {\n",
    "            \"LinearRegression\":             linear_model.LinearRegression(),                        # LinearRegression([…])\tOrdinary least squares Linear Regression.\n",
    "\n",
    "            \"ARDRegression\":                linear_model.ARDRegression(),                           #  ARDRegression([n_iter, tol, …])\tBayesian ARD regression.\n",
    "            \"BayesianRidge\":                linear_model.BayesianRidge(),                           # BayesianRidge([n_iter, tol, …])\tBayesian ridge regression.\n",
    "\n",
    "            \"HuberRegressor\":               linear_model.HuberRegressor(),                          # HuberRegressor([epsilon, …])\tLinear regression model that is robust to outliers.\n",
    "            \"OrthogonalMatchingPursuitCV\":  linear_model.OrthogonalMatchingPursuitCV(cv=5),         # OrthogonalMatchingPursuitCV([…])\tCross-validated Orthogonal Matching Pursuit model (OMP).\n",
    "            \"Perceptron\":                   linear_model.Perceptron(max_iter=1000, tol=1e-3),       # Perceptron([penalty, alpha, …])\tRead more in the User Guide.\n",
    "            \"RANSACRegressor\":              linear_model.RANSACRegressor(),                         # RANSACRegressor([…])\tRANSAC (RANdom SAmple Consensus) algorithm.\n",
    "            \"SGDRegressor\":                 linear_model.SGDRegressor(max_iter=1000, tol=1e-3),     # SGDRegressor([loss, penalty, …])\tLinear model fitted by minimizing a regularized empirical loss with SGD\n",
    "            \"TheilSenRegressor\":            linear_model.TheilSenRegressor(),                       # TheilSenRegressor([…])\tTheil-Sen Estimator: robust multivariate regression model.\n",
    "            \"PassiveAggressiveRegressor\":   linear_model.PassiveAggressiveRegressor(max_iter=1000, tol=1e-3),      # PassiveAggressiveRegressor([C, …])\tPassive Aggressive Regressor\n",
    "\n",
    "            \"Lars\":                         linear_model.Lars(eps=0.01),                            # Lars([fit_intercept, verbose, …])\tLeast Angle Regression model a.k.a.\n",
    "            \"LarsCV\":                       linear_model.LarsCV(cv=5, eps=0.01),                    # LarsCV([fit_intercept, …])\tCross-validated Least Angle Regression model.\n",
    "            \"Lasso\":                        linear_model.Lasso(alpha=1, max_iter=1000),             # Lasso([alpha, fit_intercept, …])\tLinear Model trained with L1 prior as regularizer (aka the Lasso)\n",
    "            \"LassoCV\":                      linear_model.LassoCV(cv=5),                             # LassoCV([eps, n_alphas, …])\tLasso linear model with iterative fitting along a regularization path.\n",
    "            \"LassoLars\":                    linear_model.LassoLars(eps=0.01),                       # LassoLars([alpha, …])\tLasso model fit with Least Angle Regression a.k.a.\n",
    "            \"LassoLarsCV\":                  linear_model.LassoLarsCV(cv=5, eps=0.01, max_iter=100), # LassoLarsCV([fit_intercept, …])\tCross-validated Lasso, using the LARS algorithm.\n",
    "            \"LassoLarsIC\":                  linear_model.LassoLarsIC(eps=0.01),                     # LassoLarsIC([criterion, …])\tLasso model fit with Lars using BIC or AIC for model selection\n",
    "\n",
    "            \"Ridge\":                        linear_model.Ridge(),                                   # Ridge([alpha, fit_intercept, …])\tLinear least squares with l2 regularization.\n",
    "            \"RidgeClassifier\":              linear_model.RidgeClassifier(),                         # RidgeClassifier([alpha, …])\tClassifier using Ridge regression.\n",
    "            \"RidgeClassifierCV\":            linear_model.RidgeClassifierCV(cv=5),                   # RidgeClassifierCV([alphas, …])\tRidge classifier with built-in cross-validation.\n",
    "            \"RidgeCV\":                      linear_model.RidgeCV(cv=5),                             # RidgeCV([alphas, …])\tRidge regression with built-in cross-validation.\n",
    "            \"SGDClassifier\":                linear_model.SGDClassifier(max_iter=1000, tol=1e-3),    # SGDClassifier([loss, penalty, …])\tLinear classifiers (SVM, logistic regression, a.o.) with SGD training.\n",
    "            \n",
    "            ### Ignore these\n",
    "            # \"LogisticRegression\":           linear_model.LogisticRegression(),                    # LogisticRegression([penalty, …])\tLogistic Regression (aka logit, MaxEnt) classifier.\n",
    "            # \"LogisticRegressionCV\":         linear_model.LogisticRegressionCV(cv=5),              # LogisticRegressionCV([Cs, …])\tLogistic Regression CV (aka logit, MaxEnt) classifier.\n",
    "            # \"MultiTaskLasso\":               linear_model.MultiTaskLasso(),                        # MultiTaskLasso([alpha, …])\tMulti-task Lasso model trained with L1/L2 mixed-norm as regularizer.\n",
    "            # \"MultiTaskElasticNet\":          linear_model.MultiTaskElasticNet(),                   # MultiTaskElasticNet([alpha, …])\tMulti-task ElasticNet model trained with L1/L2 mixed-norm as regularizer\n",
    "            # \"MultiTaskLassoCV\":             linear_model.MultiTaskLassoCV(cv=5),                  # MultiTaskLassoCV([eps, …])\tMulti-task Lasso model trained with L1/L2 mixed-norm as regularizer.\n",
    "            # \"MultiTaskElasticNetCV\":        linear_model.MultiTaskElasticNetCV(cv=5),             # MultiTaskElasticNetCV([…])\tMulti-task L1/L2 ElasticNet with built-in cross-validation.\n",
    "            # \"OrthogonalMatchingPursuit\":    linear_model.OrthogonalMatchingPursuit(),             # OrthogonalMatchingPursuit([…])\tOrthogonal Matching Pursuit model (OMP)\n",
    "            # \"PassiveAggressiveClassifier\":  linear_model.PassiveAggressiveClassifier(),           # PassiveAggressiveClassifier([…])\tPassive Aggressive Classifier\n",
    "        }\n",
    "\n",
    "class MultiModelLinear(MultiModel, LinearRegressionModel): pass\n",
    "class MultiModelFeatures(MultiModel, FeatureEncoding): pass\n",
    "class MultiModelPolynomial(MultiModel, PolynomialFeatureEncoding): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ time ./src/models/MultiModel.py\n",
    "\n",
    "(0.1619, 'MultiModelFeatures', 'ARDRegression')\n",
    "(0.1619, 'MultiModelFeatures', 'BayesianRidge')\n",
    "(0.162, 'MultiModelFeatures', 'RidgeCV')\n",
    "(0.1692, 'MultiModelFeatures', 'LassoLars')\n",
    "(0.1703, 'MultiModelFeatures', 'Ridge')\n",
    "(0.1727, 'MultiModelFeatures', 'LarsCV')\n",
    "(0.1727, 'MultiModelFeatures', 'LarsCV')\n",
    "(0.1741, 'MultiModelFeatures', 'TheilSenRegressor')\n",
    "(0.1758, 'MultiModelFeatures', 'LassoLars')\n",
    "(0.1763, 'MultiModelFeatures', 'LassoLarsIC')\n",
    "(0.1805, 'MultiModelFeatures', 'Lasso')\n",
    "(0.1823, 'MultiModelFeatures', 'LinearRegression')\n",
    "(0.1835, 'MultiModelLinear', 'LarsCV')\n",
    "(0.1835, 'MultiModelLinear', 'LarsCV')\n",
    "(0.1857, 'MultiModelFeatures', 'OrthogonalMatchingPursuitCV')\n",
    "(0.189, 'MultiModelPolynomial', 'LarsCV')\n",
    "(0.1908, 'MultiModelLinear', 'LassoLarsCV')\n",
    "(0.1908, 'MultiModelLinear', 'LassoLarsCV')\n",
    "(0.1909, 'MultiModelLinear', 'ARDRegression')\n",
    "(0.1909, 'MultiModelPolynomial', 'LassoLarsIC')\n",
    "(0.1911, 'MultiModelLinear', 'LassoLarsIC')\n",
    "(0.1929, 'MultiModelLinear', 'RidgeCV')\n",
    "(0.1938, 'MultiModelLinear', 'RANSACRegressor')\n",
    "(0.1939, 'MultiModelLinear', 'LassoLars')\n",
    "(0.194, 'MultiModelLinear', 'Ridge')\n",
    "(0.1942, 'MultiModelLinear', 'Lasso')\n",
    "(0.1942, 'MultiModelLinear', 'LinearRegression')\n",
    "(0.1948, 'MultiModelPolynomial', 'LassoLarsCV')\n",
    "(0.2039, 'MultiModelFeatures', 'RANSACRegressor')\n",
    "(0.2059, 'MultiModelLinear', 'Lars')\n",
    "(0.2069, 'MultiModelPolynomial', 'ARDRegression')\n",
    "(0.2082, 'MultiModelFeatures', 'HuberRegressor')\n",
    "(0.2089, 'MultiModelPolynomial', 'OrthogonalMatchingPursuitCV')\n",
    "(0.2098, 'MultiModelLinear', 'BayesianRidge')\n",
    "(0.2124, 'MultiModelLinear', 'TheilSenRegressor')\n",
    "(0.2128, 'MultiModelFeatures', 'LassoCV')\n",
    "(0.2128, 'MultiModelLinear', 'LassoCV')\n",
    "(0.2131, 'MultiModelLinear', 'HuberRegressor')\n",
    "(0.2174, 'MultiModelFeatures', 'LassoLarsCV')\n",
    "(0.2224, 'MultiModelLinear', 'OrthogonalMatchingPursuitCV')\n",
    "(0.224, 'MultiModelFeatures', 'LassoLarsCV')\n",
    "(0.2242, 'MultiModelFeatures', 'ElasticNetCV')\n",
    "(0.2245, 'MultiModelLinear', 'ElasticNetCV')\n",
    "(0.2321, 'MultiModelPolynomial', 'LassoLars')\n",
    "(0.2321, 'MultiModelPolynomial', 'LassoLars')\n",
    "(0.238, 'MultiModelPolynomial', 'Lasso')\n",
    "(0.3086, 'MultiModelLinear', 'RidgeClassifier')\n",
    "(0.3115, 'MultiModelFeatures', 'RidgeClassifierCV')\n",
    "(0.3123, 'MultiModelLinear', 'RidgeClassifierCV')\n",
    "(0.3163, 'MultiModelFeatures', 'RidgeClassifier')\n",
    "(0.3426, 'MultiModelFeatures', 'PassiveAggressiveRegressor')\n",
    "(0.3766, 'MultiModelPolynomial', 'Ridge')\n",
    "(0.3773, 'MultiModelPolynomial', 'RidgeClassifier')\n",
    "(0.3773, 'MultiModelPolynomial', 'RidgeClassifierCV')\n",
    "(0.3818, 'MultiModelPolynomial', 'RidgeCV')\n",
    "(0.3929, 'MultiModelPolynomial', 'ElasticNetCV')\n",
    "(0.3929, 'MultiModelPolynomial', 'LassoCV')\n",
    "(0.3929, 'MultiModelPolynomial', 'LassoCV')\n",
    "(0.3939, 'MultiModelPolynomial', 'BayesianRidge')\n",
    "(0.4772, 'MultiModelPolynomial', 'Perceptron')\n",
    "(0.4973, 'MultiModelPolynomial', 'PassiveAggressiveRegressor')\n",
    "(0.5944, 'MultiModelLinear', 'PassiveAggressiveRegressor')\n",
    "(0.6264, 'MultiModelPolynomial', 'LinearRegression')\n",
    "(0.6705, 'MultiModelPolynomial', 'TheilSenRegressor')\n",
    "(0.8323, 'MultiModelLinear', 'Perceptron')\n",
    "(0.8382, 'MultiModelFeatures', 'Perceptron')\n",
    "(0.9913, 'MultiModelPolynomial', 'RANSACRegressor')\n",
    "(1.2163, 'MultiModelPolynomial', 'Lars')\n",
    "(9.9689, 'MultiModelPolynomial', 'HuberRegressor')\n",
    "(11.167, 'MultiModelFeatures', 'Lars')\n",
    "(25.5386, 'MultiModelLinear', 'SGDRegressor')\n",
    "(26.4576, 'MultiModelFeatures', 'SGDRegressor')\n",
    "(35.2046, 'MultiModelPolynomial', 'SGDRegressor')\n",
    "\n",
    "\n",
    "real    287m25.848s\n",
    "user    473m18.367s\n",
    "sys     63m18.050s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit to Kaggle\n",
    "https://www.kaggle.com/c/house-prices-advanced-regression-techniques/submissions\n",
    "\n",
    "```\n",
    "$ kaggle competitions submit -c house-prices-advanced-regression-techniques -f ./data/submissions/LarsCVPolynomial.csv -m \"LarsCV + Polynomial Features\"\n",
    "```    \n",
    "- Your submission scored 0.17628, which is an improvement of your previous score of 0.20892. Great job!\n",
    "- Kaggle Rank 3493 / 4432\n",
    "\n",
    "```\n",
    "$ kaggle competitions submit -c house-prices-advanced-regression-techniques -f ./data/submissions/LarsCVLinear.csv -m \"LarsCV\"\n",
    "```    \n",
    "- Your submission scored 0.21785, which is not an improvement of your best score. Keep trying!\n",
    "- Kaggle Rank 3493 / 4432\n",
    "\n",
    "```\n",
    "$ kaggle competitions submit -c house-prices-advanced-regression-techniques -f ./data/submissions/ARDFeatures.csv -m \"ARDFeatures + Features Encoding\"\n",
    "```    \n",
    "- Your submission scored 0.15502, which is an improvement of your previous score of 0.17628. Great job!\n",
    "- Kaggle Rank 3074 / 4375\n",
    "\n",
    "```\n",
    "$ kaggle competitions submit -c house-prices-advanced-regression-techniques -f ./data/submissions/BayesianRidgeFeatures.csv -m \"BayesianRidgeFeatures + Features Encoding\"\n",
    "```    \n",
    "- Your submission scored 0.41549, which is not an improvement of your best score. Keep trying!\n",
    "- Kaggle Rank 3074 / 4375\n",
    "- Remove this item from the shortlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "\n",
    "Lets experiment with turning some of the model parameters. \n",
    "\n",
    "What does normalize=True do?\n",
    "- ARDRegression / RidgeCV + FeaturesEncoding are the best scores and do better without Normalization\n",
    "- LassoLars / LassoLarsIC + FeaturesEncoding score identically with or without Normalization\n",
    "- RidgeCV + Polynomial gets a huge improvement with normalization (0.3818 -> 0.1782)\n",
    "- LinearRegression + Normalization tends to make things worse (worst score 2.3816)\n",
    "\n",
    "RidgeCVNormalize + Polynomial is a kaggle submission entry "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.1619, 'RegularizationModelFeatures', 'ARDRegression')\n",
      "(0.162, 'RegularizationModelFeatures', 'RidgeCV')\n",
      "(0.1758, 'RegularizationModelFeatures', 'LassoLars')\n",
      "(0.1758, 'RegularizationModelFeatures', 'LassoLarsNormalize')\n",
      "(0.1763, 'RegularizationModelFeatures', 'LassoLarsIC')\n",
      "(0.1763, 'RegularizationModelFeatures', 'LassoLarsICNormalize')\n",
      "(0.1782, 'RegularizationModelPolynomial', 'RidgeCVNormalize')\n",
      "(0.1823, 'RegularizationModelFeatures', 'LinearRegression')\n",
      "(0.1871, 'RegularizationModelLinear', 'RidgeCVNormalize')\n",
      "(0.1909, 'RegularizationModelLinear', 'ARDRegression')\n",
      "(0.1909, 'RegularizationModelPolynomial', 'LassoLarsIC')\n",
      "(0.1909, 'RegularizationModelPolynomial', 'LassoLarsICNormalize')\n",
      "(0.1911, 'RegularizationModelLinear', 'LassoLarsIC')\n",
      "(0.1911, 'RegularizationModelLinear', 'LassoLarsICNormalize')\n",
      "(0.1929, 'RegularizationModelLinear', 'RidgeCV')\n",
      "(0.1939, 'RegularizationModelLinear', 'LassoLars')\n",
      "(0.1939, 'RegularizationModelLinear', 'LassoLarsNormalize')\n",
      "(0.1942, 'RegularizationModelLinear', 'LinearRegression')\n",
      "(0.1942, 'RegularizationModelLinear', 'LinearRegressionNormalize')\n",
      "(0.2, 'RegularizationModelFeatures', 'RidgeCVNormalize')\n",
      "(0.2069, 'RegularizationModelPolynomial', 'ARDRegression')\n",
      "(0.2321, 'RegularizationModelPolynomial', 'LassoLars')\n",
      "(0.2321, 'RegularizationModelPolynomial', 'LassoLarsNormalize')\n",
      "(0.3114, 'RegularizationModelPolynomial', 'LinearRegressionNormalize')\n",
      "(0.3818, 'RegularizationModelPolynomial', 'RidgeCV')\n",
      "(0.3929, 'RegularizationModelLinear', 'ARDRegressionNormalize')\n",
      "(0.3929, 'RegularizationModelFeatures', 'ARDRegressionNormalize')\n",
      "(0.3929, 'RegularizationModelPolynomial', 'ARDRegressionNormalize')\n",
      "(0.6264, 'RegularizationModelPolynomial', 'LinearRegression')\n",
      "(2.3816, 'RegularizationModelFeatures', 'LinearRegressionNormalize')\n"
     ]
    }
   ],
   "source": [
    "class RegularizationModel():\n",
    "    @cached_property\n",
    "    def models( self ) -> Dict[str, LinearModel]:\n",
    "        return {\n",
    "            \"LinearRegression\":             linear_model.LinearRegression(),                        # LinearRegression([…])\tOrdinary least squares Linear Regression.\n",
    "            \"RidgeCV\":                      linear_model.RidgeCV(cv=5),                             # RidgeCV([alphas, …])\tRidge regression with built-in cross-validation.\n",
    "            \"LassoLars\":                    linear_model.LassoLars(eps=0.01),                       # LassoLars([alpha, …])\tLasso model fit with Least Angle Regression a.k.a.\n",
    "            \"LassoLarsIC\":                  linear_model.LassoLarsIC(eps=0.01),                     # LassoLarsIC([criterion, …])\tLasso model fit with Lars using BIC or AIC for model selection\n",
    "            \"ARDRegression\":                linear_model.ARDRegression(),                           # ARDRegression([n_iter, tol, …])\tBayesian ARD regression.\n",
    "\n",
    "            \"LinearRegressionNormalize\":    linear_model.LinearRegression(normalize=True),          # LinearRegression([…])\tOrdinary least squares Linear Regression.\n",
    "            \"RidgeCVNormalize\":             linear_model.RidgeCV(cv=5, normalize=True),             # RidgeCV([alphas, …])\tRidge regression with built-in cross-validation.\n",
    "            \"LassoLarsNormalize\":           linear_model.LassoLars(eps=0.01, normalize=True),       # LassoLars([alpha, …])\tLasso model fit with Least Angle Regression a.k.a.\n",
    "            \"LassoLarsICNormalize\":         linear_model.LassoLarsIC(eps=0.01, normalize=True),     # LassoLarsIC([criterion, …])\tLasso model fit with Lars using BIC or AIC for model selection\n",
    "            \"ARDRegressionNormalize\":       linear_model.ARDRegression(normalize=True),             # ARDRegression([n_iter, tol, …])\tBayesian ARD regression.\n",
    "        }\n",
    "\n",
    "class RegularizationModelLinear(     RegularizationModel, LinearRegressionModel     ): pass\n",
    "class RegularizationModelFeatures(   RegularizationModel, FeatureEncoding           ): pass\n",
    "class RegularizationModelPolynomial( RegularizationModel, PolynomialFeatureEncoding ): pass\n",
    "\n",
    "results = sorted(pydash.flatten([\n",
    "    RegularizationModelLinear().model_scores_list(),\n",
    "    RegularizationModelFeatures().model_scores_list(),\n",
    "    RegularizationModelPolynomial().model_scores_list(),\n",
    "]), key=itemgetter(0))\n",
    "for result in results: print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit to Kaggle\n",
    "https://www.kaggle.com/c/house-prices-advanced-regression-techniques/submissions\n",
    "\n",
    "```\n",
    "(0.1782, 'RidgeCVNormalizePolynomial', 'RidgeCVNormalizePolynomial', 'X_feature_exclude X_feature_year_ages X_feature_label_encode X_feature_onehot X_feature_polynomial')\n",
    "```\n",
    "\n",
    "```\n",
    "$ kaggle competitions submit -c house-prices-advanced-regression-techniques -f ./data/submissions/RidgeCVNormalizePolynomial.csv -m \"RidgeCV + Normalize + Polynomial Features\"\n",
    "```    \n",
    "- Your submission scored 0.18379, which is not an improvement of your best score. Keep trying!\n",
    "- Kaggle Rank 3076 / 4433"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "(0.162, 'RegularizationModelFeatures', 'RidgeCV'),\n",
    "(0.1823, 'RegularizationModelFeatures', 'LinearRegression'),\n",
    "(0.1929, 'RegularizationModelLinear', 'RidgeCV'),\n",
    "(0.1942, 'RegularizationModelLinear', 'LinearRegression'),\n",
    "(0.2128, 'RegularizationModelLinear', 'LassoCV'),\n",
    "(0.2128, 'RegularizationModelFeatures', 'LassoCV'),\n",
    "(0.2242, 'RegularizationModelFeatures', 'ElasticNetCV'),\n",
    "(0.2245, 'RegularizationModelLinear', 'ElasticNetCV'),\n",
    "(0.3818, 'RegularizationModelPolynomial', 'RidgeCV'),\n",
    "(0.3929, 'RegularizationModelPolynomial', 'ElasticNetCV'),\n",
    "(0.3929, 'RegularizationModelPolynomial', 'LassoCV'),\n",
    "(0.6264, 'RegularizationModelPolynomial', 'LinearRegression')\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
