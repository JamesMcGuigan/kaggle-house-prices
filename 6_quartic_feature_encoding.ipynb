{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Competition - House Prices: Advanced Regression Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import RegularizationModelLinear, RegularizationModelFeatures, RegularizationModelPolynomial, RegularizationModelSquared\n",
    "from src.models import SquaredFeatureEncoding\n",
    "from src.models import MultiModelSquared\n",
    "\n",
    "import pydash\n",
    "import warnings\n",
    "from operator import itemgetter\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quartic Feature Encoding\n",
    "\n",
    "Whilst sklearn.preprocessing.PolynomialFeatures() generates a very large number of cross-pollination polynomial features, \n",
    "linear regression might be better servered through using just the self-square features ^2 + ^3 + ^4\n",
    "\n",
    "Optimial tuning parameter for this dataset is 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.1507, 'SquaredFeatureEncoding', 4),\n",
       " (0.1532, 'SquaredFeatureEncoding', 3),\n",
       " (0.1629, 'SquaredFeatureEncoding', 2),\n",
       " (0.1942, 'SquaredFeatureEncoding', 1),\n",
       " (0.2231, 'SquaredFeatureEncoding', 8),\n",
       " (0.2516, 'SquaredFeatureEncoding', 5),\n",
       " (0.3344, 'SquaredFeatureEncoding', 6),\n",
       " (0.3696, 'SquaredFeatureEncoding', 7),\n",
       " (0.5343, 'SquaredFeatureEncoding', 9)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "for n in range(1,10):\n",
    "    result = SquaredFeatureEncoding( **{ 'X_feature_squared': n } ).summary()\n",
    "    results.append( (result[0], result[1], n) )\n",
    "sorted( results )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing this against the range of scikitlearn.linear_model's (next notebook), we discovered:\n",
    "- Squared/Quartic feature encoding outperforms most PolynomialFeatures encoding \n",
    "- Squared/Quartic feature encoding works best with LassoLars + ElasticNet\n",
    "- X_feature_squared tuning parameter doesn't improve when using MultiModel selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.1386, 'MultiModelSquared', 'LassoLars'),\n",
       " (0.1389, 'MultiModelSquared', 'ElasticNet'),\n",
       " (0.1399, 'MultiModelSquared', 'Lasso'),\n",
       " (0.1402, 'MultiModelSquared', 'RidgeCV'),\n",
       " (0.1413, 'MultiModelSquared', 'Ridge'),\n",
       " (0.1507, 'MultiModelSquared', 'LinearRegression'),\n",
       " (0.1581, 'MultiModelSquared', 'LassoLarsCV'),\n",
       " (0.1581, 'MultiModelSquared', 'LassoLarsIC'),\n",
       " (0.1659, 'MultiModelSquared', 'ARDRegression'),\n",
       " (0.1785, 'MultiModelSquared', 'OrthogonalMatchingPursuitCV'),\n",
       " (0.1801, 'MultiModelSquared', 'BayesianRidge'),\n",
       " (0.2526, 'MultiModelSquared', 'TheilSenRegressor'),\n",
       " (0.3013, 'MultiModelSquared', 'RidgeClassifierCV'),\n",
       " (0.3063, 'MultiModelSquared', 'RidgeClassifier'),\n",
       " (0.3906, 'MultiModelSquared', 'LassoCV'),\n",
       " (0.3906, 'MultiModelSquared', 'ElasticNetCV'),\n",
       " (0.3929, 'MultiModelSquared', 'LarsCV'),\n",
       " (0.4008, 'MultiModelSquared', 'Perceptron'),\n",
       " (0.4252, 'MultiModelSquared', 'RANSACRegressor'),\n",
       " (0.5673, 'MultiModelSquared', 'PassiveAggressiveRegressor'),\n",
       " (0.6503, 'MultiModelSquared', 'SGDClassifier'),\n",
       " (5.6472, 'MultiModelSquared', 'Lars'),\n",
       " (6.4146, 'MultiModelSquared', 'HuberRegressor'),\n",
       " (82.1952, 'MultiModelSquared', 'SGDRegressor')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MultiModelSquared(X_feature_squared=4).model_scores_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.1386, 'RegularizationModelSquared', 'LassoLars')\n",
      "(0.1389, 'RegularizationModelSquared', 'ElasticNet')\n",
      "(0.1402, 'RegularizationModelSquared', 'RidgeCV')\n",
      "(0.1507, 'RegularizationModelSquared', 'LinearRegression')\n",
      "(0.1581, 'RegularizationModelSquared', 'LassoLarsIC')\n",
      "(0.1619, 'RegularizationModelFeatures', 'ARDRegression')\n",
      "(0.162, 'RegularizationModelFeatures', 'RidgeCV')\n",
      "(0.1659, 'RegularizationModelSquared', 'ARDRegression')\n",
      "(0.1688, 'RegularizationModelFeatures', 'ElasticNet')\n",
      "(0.1758, 'RegularizationModelFeatures', 'LassoLars')\n",
      "(0.1763, 'RegularizationModelFeatures', 'LassoLarsIC')\n",
      "(0.1823, 'RegularizationModelFeatures', 'LinearRegression')\n",
      "(0.1899, 'RegularizationModelLinear', 'ElasticNet')\n",
      "(0.1909, 'RegularizationModelLinear', 'ARDRegression')\n",
      "(0.1909, 'RegularizationModelPolynomial', 'LassoLarsIC')\n",
      "(0.1911, 'RegularizationModelLinear', 'LassoLarsIC')\n",
      "(0.1929, 'RegularizationModelLinear', 'RidgeCV')\n",
      "(0.1939, 'RegularizationModelLinear', 'LassoLars')\n",
      "(0.1942, 'RegularizationModelLinear', 'LinearRegression')\n",
      "(0.2069, 'RegularizationModelPolynomial', 'ARDRegression')\n",
      "(0.2321, 'RegularizationModelPolynomial', 'LassoLars')\n",
      "(0.247, 'RegularizationModelPolynomial', 'ElasticNet')\n",
      "(0.3818, 'RegularizationModelPolynomial', 'RidgeCV')\n",
      "(0.6264, 'RegularizationModelPolynomial', 'LinearRegression')\n"
     ]
    }
   ],
   "source": [
    "results = sorted(pydash.flatten([\n",
    "    RegularizationModelLinear().model_scores_list(),\n",
    "    RegularizationModelFeatures().model_scores_list(),\n",
    "    RegularizationModelPolynomial().model_scores_list(),\n",
    "    RegularizationModelSquared().model_scores_list(),\n",
    "]), key=itemgetter(0))\n",
    "for result in results: print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit to Kaggle\n",
    "https://www.kaggle.com/c/house-prices-advanced-regression-techniques/submissions\n",
    "\n",
    "```\n",
    "(0.1386, 'LassoLarsSquared', 'LassoLarsSquared', 'X_feature_exclude X_feature_year_ages X_feature_label_encode X_feature_onehot X_feature_squared')\n",
    "(0.1389, 'ElasticNetSquared', 'ElasticNetSquared', 'X_feature_exclude X_feature_year_ages X_feature_label_encode X_feature_onehot X_feature_squared')\n",
    "(0.1507, 'SquaredFeatureEncoding', 'LinearRegression', 'X_feature_exclude X_feature_year_ages X_feature_label_encode X_feature_onehot X_feature_squared')\n",
    "```\n",
    "\n",
    "```\n",
    "$ kaggle competitions submit -c house-prices-advanced-regression-techniques -f ./data/submissions/SquaredFeatureEncoding.csv -m \"Quartic Features\"\n",
    "```    \n",
    "- Your submission scored 2.29145, which is not an improvement of your best score. Keep trying!\n",
    "\n",
    "```\n",
    "$ kaggle competitions submit -c house-prices-advanced-regression-techniques -f ./data/submissions/ElasticNetSquared.csv -m \"ElasticNet + Quartic Features\"\n",
    "```    \n",
    "- Your submission scored 2.42165, which is not an improvement of your best score. Keep trying!\n",
    "\n",
    "```\n",
    "$ kaggle competitions submit -c house-prices-advanced-regression-techniques -f ./data/submissions/LassoLarsSquared.csv -m \"LassoLars + Quartic Features\"\n",
    "```    \n",
    "- Your submission scored 2.42903, which is not an improvement of your best score. Keep trying!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is weird!\n",
    "\n",
    "Whilst Squared/Quartic Features produced very good results on the local dataset, the model was completely useless on Kaggle"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
